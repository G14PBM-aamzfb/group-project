\documentclass{article}

\begin{document}
\section{Simulation}
Computer simulation of continuous mathematical equations need to account for the fact that computers can only represent quanitities with finite accuracy. This is achieved by discretizing time and space into a grid, and applying adequate methods to step between the points in the grid by using current values to estimate values at the next position.

\subsection{Discretization Schemes}

As outlined earlier, we used a Runge-Kutta scheme for temporal discretization and Finite Difference schemes for spatial discretization.

\subsubsection*{Runge Kutta}

The Runge-Kutta methods improve upon the idea of Euler's method to numerically approximate solutions for initial value problems by estimating intermediate rates of change within a time step. For an ODE $y' = f(y,x,t)$ hey are formulated as

$$y_{n+1} = y_n + h \sum_{j=1}^s b_j k_j$$

where $y_n, y_{n+1}$ are the current and next value in the solution, h is the time step,  $k_j$ and $b_j$ are the intermediate rates of change and their coefficients according to the specific order of the scheme. The intermediate rates are

$$k_j = f\left(t_n + h c_j, y_n + h \sum_{l=1}^{j-1} a_{jl} k_l \right),\,j=1,...,s$$

In our simulation we used the second order Runge-Kutta method, RK2, to integrate over the value of time derivatives that we determined using the finite difference method. Its intermediates steps are

$$k_1 = f(t_n, y_n )$$
$$k_2 = f \left( t_n+\frac{h}{2}, y_n+\frac{h}{2}k_1 \right)$$

\subsubsection*{Finite Difference}
The finite difference methods for stepping through space are based on the Taylor series expansion of the function:

$$f(x+h) = \sum_{n=0}^\infty \frac{h^n}{n!}f^{(n)}(x)$$

Truncating the series after the n-th element adds an error of order $O(h^{(n)})$ [O-notation] to the approximation and applying the truncation for example after n=2 results in 

$$f(x+h)=f(x)+hf'(x)+O(h^2)$$

Which suggests a discrete-time approximation for the derivative of $f(x)$:

$$f'(x)=\frac{f(x+h)-f(x)}{h}+O(h)$$

Where the term $\Delta_h[f](x)=f(x+h)-f(x)$ is known as the forward-difference. Other variants are central- (a) and backward-difference (b): Other variants are central- (a) and backward-difference (b):

$$\delta_h[f](x)=f(x+\frac{1}{2}h)-f(x-\frac{1}{2}h)$$

$$\nabla_h[f](x) = f(x)-f(x-h)$$

Applying these approximations recursively yields analogous approximations for higher-order derivatives, e.g. the second order derivative using central-difference:

$$f''(x)=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h^2)$$


\subsection{Numerical stability}

A key property of numerical methods is that with decreasing tim-step size they must converge towards the continuous function they are meant to approximate. I.e. the largest deviation of the numerical approximations $x_n$ from its respective exact value $x(t_n)$tends to 0 as time-step size $dt$ goes to 0:

$$\epsilon_N=max_{1\le n\le N}||x_n-x(t_n)||\to0\ \ as\ \ dt\to 0$$

Since the exact values are usually not available - the whole reason for the approximation - we substitute the requirement for convergence with 2 equivalent requirements - consistency and stability. Their equivalence has been proven for finite difference schemes in [Lax 1956] and is thus applicable to our simulation.

Consistency requires that the error $e_n$ introduced at a single time step of the numerical method $\Phi$ tends to 0 as $dt$ goes to 0:

$$e_n = x(t_n+dt) - \Phi(x(t_n),t_n,dt),\ \ \sum_{n=1}^N|e_n|\to 0\ \mathrm{as}\ dt\to 0$$

 Since the truncation error $O(\cdot)$ in our simulation is a polynomial in $dt$ this criteria holds.

 
 Stability is the requirement that the error due to approximation does not increase faster than linearly with time. 

To determine the stability of a finite difference scheme we can employ the von Neumann stability analysis. The method makes use of a representation of the solution of the PDE as a Fourier series expansion $u_j^k=r^ke^{ij\beta \Delta x}$, where both pairs, the time index $k$ in the LHS and the exponent $k$ of the amplification factor $r$, as well as the spatial index $j$ on both sides of the equation, have the same values, respectively. Substituting the RHS into a finite difference scheme yields, for a 1D linear diffusion equation with central difference:

$$\frac{u_j^{k+1}-u_j^k}{\Delta t} = D\frac{r^ke^{i(j+1)\beta \Delta x}-2r^ke^{ij\beta \Delta x}+r^ke^{i(j-1)\beta \Delta x}}{\Delta x^2}$$

\medskip
which can be simplified to
\smallskip

$$r=1+\frac{D \Delta t}{\Delta x^2}(e^{i\beta \Delta x}+e^{-i\beta \Delta x}-2)\ \to\ r=1-4\frac{D \Delta t}{\Delta x^2} \mathrm{sin}^2(\beta\Delta x/2)$$
\smallskip

yielding $\Delta t < \frac{\Delta x^2}{2D}$ as the condition for stability. Our attempts to apply this analysis to our system of equations failed a) because of the nonlinearities and b) it being a system rather than a simple PDE (i.e. V occurs in the equation for $U_t$).

\subsection{Implementation}

In the paper Garfinkel, et al. inform us that they used a finite difference method with second order Runge Kutta. To allow for comparison between their results and ours, we implemented our simulations in Python using the aforementioned methods.

Utilizing the efficient matrix operations in Numpy we were formulated the different schemes as arithmetic operations on the shifted spatial grid, e.g. $u_{j-1}-2u_j+u_{j+1}$ becomes \texttt{U[:,0:-2]-2*U[:,1:-1]+U[:,2:0]}. We were able to retrace the simulations and results presented in the article, using an explicit forward time, central difference scheme (FTCS).

\subsection{Encountered issues}

In the supplements to the article Garfinkel et al. described for the cell density as

$$\frac{\partial N}{\partial t}=\nabla^2N-\frac{\chi_0}{1+U^2}\nabla U \nabla N- \nabla(\frac{\chi_0}{1+U^2}\nabla U)N$$

Despite best efforts, we did not succeed in simulating the nonlinear PDE. Following the suggestion of our advisor, Dr. Reuben O'dea, we tried to mitigate the appearing numerical instability using an upwind scheme, but were not able to prevent blow-up in the simulation.

[REMOVE]

The idea behind the upwind scheme is to deal with the advection term in the PDE by compensating for the acceleration caused by advection in the characteristics between $u^n$ and $u^{n+1}$ in order to assure that the domain of numerical dependence contains the domain of analytical dependence.

[REMOVE]

\pagebreak

\section{Results}

We decided to first check the central assertions made in the original paper. The authors suggest that the appearance of Turing patterns is critically dependent on the ratio of the morphogens' diffusion constants, $D_u$ having to be smaller than $D_v$. Varying the value of $D_v$ while keeping $D_u$ fixed at 0.01 we observed the disappearance of Turing patterns as $D_v$ approaches and passes that of $D_u$ (Fig. 1).

\begin{figure}[H]
  \includegraphics[width=\linewidth]{diffusivity.png}
  \caption{Varying Diffusivity}
  \label{fig:dv}
\end{figure}

Next we tested the assertion that the width of the stripes in the Turing pattern varies with changes in the timescale $\gamma$, where a larger $\gamma$ is expected to result in thinner stripes. The simulations in Fig. 2 confirm this.

\begin{figure}[H]
  \includegraphics[width=\linewidth]{timescale.png}
  \caption{Figure 2: Varying Timescale}
  \label{fig:gamma}
\end{figure}

Most importantly, Garfinkel et al. describe a qualitative variation in the appearing patterns depending on the magnitude of the external inhibition S. Garfinkel described this in more rigorous terms in []. We were indeed able to confirm the distinct types of patterns across the parameter space. Varying the parameter $S$ from 0 to 0.5 we discovered that the system exhibited largely uniform states with holes and spots at the low and high end of the interval, respectively. In the range between we observed the appearance of a negative pattern that inverted around the value $S=0.125$ to display a positive labyrnth pattern, transitioning through a region (~[0.3,0.4]) displaying fragmented stripes into spots.

\begin{figure}
  \includegraphics[width=\linewidth]{inhibition.png}
  \caption{Figure 1: Varying external inhibition}
  \label{fig:s}
\end{figure}



\end{document}
